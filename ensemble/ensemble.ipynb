{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5-D2V8NYTNH",
        "outputId": "33fba9c4-4da0-4aeb-bc3a-b0c3708fde5d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, LearningCurveDisplay, ShuffleSplit\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from scipy import stats\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import joblib\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.evaluate import bias_variance_decomp"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8X6s7rJSjgf8"
      },
      "source": [
        "# Implement different classifiers, take majority vote, aka: **Bagging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XWNNdN0qIXh",
        "outputId": "ae266347-458b-4b4b-af48-a1784ed56880"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Body_Level\n",
              "Body Level 4    680\n",
              "Body Level 3    406\n",
              "Body Level 2    201\n",
              "Body Level 1    190\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('../body_level_classification_train.csv')\n",
        "df.Body_Level.value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QxeUF0DO1ItE"
      },
      "source": [
        "### Some Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "h1zZssqwq6Yh",
        "outputId": "6ce02c7b-3daf-4cb6-c73a-4c6edbacfc65"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>H_Cal_Consump</th>\n",
              "      <th>Veg_Consump</th>\n",
              "      <th>Water_Consump</th>\n",
              "      <th>Alcohol_Consump</th>\n",
              "      <th>Smoking</th>\n",
              "      <th>Meal_Count</th>\n",
              "      <th>Food_Between_Meals</th>\n",
              "      <th>Fam_Hist</th>\n",
              "      <th>H_Cal_Burn</th>\n",
              "      <th>Phys_Act</th>\n",
              "      <th>Time_E_Dev</th>\n",
              "      <th>Transport</th>\n",
              "      <th>Body_Level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Female</td>\n",
              "      <td>22.547298</td>\n",
              "      <td>1.722461</td>\n",
              "      <td>51.881263</td>\n",
              "      <td>yes</td>\n",
              "      <td>2.663421</td>\n",
              "      <td>1.041110</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>Frequently</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0.794402</td>\n",
              "      <td>1.391948</td>\n",
              "      <td>Public_Transportation</td>\n",
              "      <td>Body Level 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>19.799054</td>\n",
              "      <td>1.743702</td>\n",
              "      <td>54.927529</td>\n",
              "      <td>yes</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.847264</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>no</td>\n",
              "      <td>3.289260</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>1.680844</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>Public_Transportation</td>\n",
              "      <td>Body Level 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Female</td>\n",
              "      <td>17.823438</td>\n",
              "      <td>1.708406</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.642241</td>\n",
              "      <td>1.099231</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>no</td>\n",
              "      <td>3.452590</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0.418875</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>Public_Transportation</td>\n",
              "      <td>Body Level 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Female</td>\n",
              "      <td>19.007177</td>\n",
              "      <td>1.690727</td>\n",
              "      <td>49.895716</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.212908</td>\n",
              "      <td>1.029703</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>no</td>\n",
              "      <td>3.207071</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>Public_Transportation</td>\n",
              "      <td>Body Level 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>19.729250</td>\n",
              "      <td>1.793315</td>\n",
              "      <td>58.195150</td>\n",
              "      <td>yes</td>\n",
              "      <td>2.508835</td>\n",
              "      <td>2.076933</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>3.435905</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>2.026668</td>\n",
              "      <td>1.443328</td>\n",
              "      <td>Automobile</td>\n",
              "      <td>Body Level 1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Gender        Age    Height     Weight H_Cal_Consump  Veg_Consump   \n",
              "0  Female  22.547298  1.722461  51.881263           yes     2.663421  \\\n",
              "1    Male  19.799054  1.743702  54.927529           yes     2.000000   \n",
              "2  Female  17.823438  1.708406  50.000000           yes     1.642241   \n",
              "3  Female  19.007177  1.690727  49.895716           yes     1.212908   \n",
              "4    Male  19.729250  1.793315  58.195150           yes     2.508835   \n",
              "\n",
              "   Water_Consump Alcohol_Consump Smoking  Meal_Count Food_Between_Meals   \n",
              "0       1.041110              no      no    3.000000         Frequently  \\\n",
              "1       2.847264       Sometimes      no    3.289260          Sometimes   \n",
              "2       1.099231       Sometimes      no    3.452590          Sometimes   \n",
              "3       1.029703       Sometimes      no    3.207071          Sometimes   \n",
              "4       2.076933              no      no    3.435905          Sometimes   \n",
              "\n",
              "  Fam_Hist H_Cal_Burn  Phys_Act  Time_E_Dev              Transport   \n",
              "0      yes         no  0.794402    1.391948  Public_Transportation  \\\n",
              "1      yes         no  1.680844    2.000000  Public_Transportation   \n",
              "2       no         no  0.418875    1.000000  Public_Transportation   \n",
              "3       no         no  2.000000    1.000000  Public_Transportation   \n",
              "4      yes         no  2.026668    1.443328             Automobile   \n",
              "\n",
              "     Body_Level  \n",
              "0  Body Level 1  \n",
              "1  Body Level 1  \n",
              "2  Body Level 1  \n",
              "3  Body Level 1  \n",
              "4  Body Level 1  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MepSMHprgFJ",
        "outputId": "90ea62a2-fc1b-4ddb-ee91-18c2b765fbc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['no' 'Sometimes' 'Frequently' 'Always']\n",
            "['no' 'yes']\n",
            "['Frequently' 'Sometimes' 'no' 'Always']\n",
            "['yes' 'no']\n",
            "['no' 'yes']\n",
            "['Body Level 1' 'Body Level 2' 'Body Level 3' 'Body Level 4']\n",
            "['Public_Transportation' 'Automobile' 'Walking' 'Bike' 'Motorbike']\n"
          ]
        }
      ],
      "source": [
        "print(df.Alcohol_Consump.unique())\n",
        "print(df.Smoking.unique())\n",
        "print(df.Food_Between_Meals.unique())\n",
        "print(df.Fam_Hist.unique())\n",
        "print(df.H_Cal_Burn.unique())\n",
        "print(df.Body_Level.unique())\n",
        "print(df.Transport.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vgIOawNyTUnr"
      },
      "outputs": [],
      "source": [
        "def AdaBoostPreprocessor(df, inference=False):\n",
        "    df.loc[df[\"Gender\"]==\"Male\", 'Gender'] = True\n",
        "    df.loc[df[\"Gender\"]==\"Female\", 'Gender'] = False\n",
        "    df.loc[df[\"H_Cal_Consump\"]==\"yes\", 'H_Cal_Consump'] = True\n",
        "    df.loc[df[\"H_Cal_Consump\"]==\"no\", 'H_Cal_Consump'] = False\n",
        "    ListAlcoholConsump = ['no', 'Sometimes', 'Frequently', 'Always']\n",
        "    df['Alcohol_Consump']=df['Alcohol_Consump'].apply(lambda x: ListAlcoholConsump.index(str(x)))\n",
        "    df.loc[df[\"Smoking\"]==\"yes\", 'Smoking'] = True\n",
        "    df.loc[df[\"Smoking\"]==\"no\", 'Smoking'] = False\n",
        "    df['Food_Between_Meals']=df['Food_Between_Meals'].apply(lambda x: ListAlcoholConsump.index(str(x)))\n",
        "    df.loc[df[\"Fam_Hist\"]==\"yes\", 'Fam_Hist'] = True\n",
        "    df.loc[df[\"Fam_Hist\"]==\"no\", 'Fam_Hist'] = False\n",
        "    df.loc[df[\"H_Cal_Burn\"]==\"yes\", 'H_Cal_Burn'] = True\n",
        "    df.loc[df[\"H_Cal_Burn\"]==\"no\", 'H_Cal_Burn'] = False\n",
        "    #df['Transport'] = LabelEncoder().fit_transform(df['Transport'])\n",
        "    ListTransport = ['Public_Transportation', 'Automobile', 'Walking', 'Bike', 'Motorbike']\n",
        "    df['Transport'] = df['Transport'].apply(lambda x: ListTransport.index(str(x)))\n",
        "\n",
        "    if(not inference):\n",
        "      #df['Body_Level'] = LabelEncoder().fit_transform(df['Body_Level'])\n",
        "      ListBodyLevel = ['Body Level 1', 'Body Level 2', 'Body Level 3', 'Body Level 4']\n",
        "      df['Body_Level'] = df['Body_Level'].apply(lambda x: ListBodyLevel.index(str(x)) + 1)\n",
        "      y=df['Body_Level']\n",
        "\n",
        "    bmi=df['Weight']/(df['Height']**2)\n",
        "    df['bmi'] = bmi\n",
        "    y0=bmi*0\n",
        "    y0[bmi>29.9]=4\n",
        "    y0[bmi<=29.9]=3\n",
        "    y0[bmi<=24.9]=2\n",
        "    y0[bmi<18.5]=1\n",
        "\n",
        "    return df\n",
        "    \n",
        "def AdaBoostSMOTE(df):\n",
        "    oversample = SMOTE()\n",
        "    y=df['Body_Level']\n",
        "    X=df.loc[:, df.columns != 'Body_Level']\n",
        "\n",
        "    X0, y = oversample.fit_resample(X, y)\n",
        "    df = X0\n",
        "    df['Body_Level'] = y\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "hDPOBk17Tm31",
        "outputId": "f2ff4a0c-8b02-4d6c-fabc-8bda1fa09abc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Veg_Consump</th>\n",
              "      <th>Water_Consump</th>\n",
              "      <th>Alcohol_Consump</th>\n",
              "      <th>Meal_Count</th>\n",
              "      <th>Food_Between_Meals</th>\n",
              "      <th>Phys_Act</th>\n",
              "      <th>Time_E_Dev</th>\n",
              "      <th>Transport</th>\n",
              "      <th>bmi</th>\n",
              "      <th>Body_Level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.000000</td>\n",
              "      <td>2720.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>23.022697</td>\n",
              "      <td>1.693889</td>\n",
              "      <td>74.727551</td>\n",
              "      <td>2.397177</td>\n",
              "      <td>1.933783</td>\n",
              "      <td>0.640074</td>\n",
              "      <td>2.730573</td>\n",
              "      <td>1.179779</td>\n",
              "      <td>1.103011</td>\n",
              "      <td>0.689512</td>\n",
              "      <td>0.270956</td>\n",
              "      <td>25.852069</td>\n",
              "      <td>2.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5.872091</td>\n",
              "      <td>0.095451</td>\n",
              "      <td>24.934293</td>\n",
              "      <td>0.534908</td>\n",
              "      <td>0.603819</td>\n",
              "      <td>0.527519</td>\n",
              "      <td>0.816209</td>\n",
              "      <td>0.487840</td>\n",
              "      <td>0.828172</td>\n",
              "      <td>0.602593</td>\n",
              "      <td>0.566084</td>\n",
              "      <td>7.684392</td>\n",
              "      <td>1.11824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>15.000000</td>\n",
              "      <td>1.456346</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.291588</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>19.000000</td>\n",
              "      <td>1.620000</td>\n",
              "      <td>54.402243</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.437703</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.634134</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.328103</td>\n",
              "      <td>0.032989</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18.613909</td>\n",
              "      <td>1.75000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>21.181029</td>\n",
              "      <td>1.700000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>2.392422</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.688176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.668843</td>\n",
              "      <td>2.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>25.018193</td>\n",
              "      <td>1.762400</td>\n",
              "      <td>87.209141</td>\n",
              "      <td>2.994391</td>\n",
              "      <td>2.346624</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.893295</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30.112553</td>\n",
              "      <td>3.25000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>55.246250</td>\n",
              "      <td>1.980000</td>\n",
              "      <td>173.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>50.811753</td>\n",
              "      <td>4.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Age       Height       Weight  Veg_Consump  Water_Consump   \n",
              "count  2720.000000  2720.000000  2720.000000  2720.000000    2720.000000  \\\n",
              "mean     23.022697     1.693889    74.727551     2.397177       1.933783   \n",
              "std       5.872091     0.095451    24.934293     0.534908       0.603819   \n",
              "min      15.000000     1.456346    39.000000     1.000000       1.000000   \n",
              "25%      19.000000     1.620000    54.402243     2.000000       1.437703   \n",
              "50%      21.181029     1.700000    70.000000     2.392422       2.000000   \n",
              "75%      25.018193     1.762400    87.209141     2.994391       2.346624   \n",
              "max      55.246250     1.980000   173.000000     3.000000       3.000000   \n",
              "\n",
              "       Alcohol_Consump   Meal_Count  Food_Between_Meals     Phys_Act   \n",
              "count      2720.000000  2720.000000         2720.000000  2720.000000  \\\n",
              "mean          0.640074     2.730573            1.179779     1.103011   \n",
              "std           0.527519     0.816209            0.487840     0.828172   \n",
              "min           0.000000     1.000000            0.000000     0.000000   \n",
              "25%           0.000000     2.634134            1.000000     0.328103   \n",
              "50%           1.000000     3.000000            1.000000     1.000000   \n",
              "75%           1.000000     3.000000            1.000000     1.893295   \n",
              "max           3.000000     4.000000            3.000000     3.000000   \n",
              "\n",
              "        Time_E_Dev    Transport          bmi  Body_Level  \n",
              "count  2720.000000  2720.000000  2720.000000  2720.00000  \n",
              "mean      0.689512     0.270956    25.852069     2.50000  \n",
              "std       0.602593     0.566084     7.684392     1.11824  \n",
              "min       0.000000     0.000000    13.291588     1.00000  \n",
              "25%       0.032989     0.000000    18.613909     1.75000  \n",
              "50%       0.688176     0.000000    24.668843     2.50000  \n",
              "75%       1.000000     0.000000    30.112553     3.25000  \n",
              "max       2.000000     4.000000    50.811753     4.00000  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = AdaBoostPreprocessor(df).copy()\n",
        "df = AdaBoostSMOTE(df).copy()\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL9FbNgBqQ0r",
        "outputId": "717ee321-9892-404d-ca01-a154171b7650"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Gender                0\n",
              "Age                   0\n",
              "Height                0\n",
              "Weight                0\n",
              "H_Cal_Consump         0\n",
              "Veg_Consump           0\n",
              "Water_Consump         0\n",
              "Alcohol_Consump       0\n",
              "Smoking               0\n",
              "Meal_Count            0\n",
              "Food_Between_Meals    0\n",
              "Fam_Hist              0\n",
              "H_Cal_Burn            0\n",
              "Phys_Act              0\n",
              "Time_E_Dev            0\n",
              "Transport             0\n",
              "bmi                   0\n",
              "Body_Level            0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "FSGuKVNCwVeV",
        "outputId": "78e03886-ded9-4b96-8935-2df5e0fabf3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ndf_no_outliers = df.loc[:, ['Age',\\t'Height',\\t'Weight'\\t,'Veg_Consump'\\t,'Water_Consump','Meal_Count',\\t'H_Cal_Burn',\\t'Phys_Act',\\t'Time_E_Dev']]\\ndf_no_outliers = df_no_outliers.astype(np.float64)\\ndf_no_outliers.sc\\n\\ndata_mean, data_std = df_no_outliers.mean(), df_no_outliers.std()\\n# identify outliers\\ncut_off = data_std * 3\\nlower, upper = data_mean - cut_off, data_mean + cut_off\\nprint(lower)\\n# identify outliers\\noutliers = [x for x in df_no_outliers if x < lower or x > upper]\\nprint('Identified outliers: %d' % len(outliers))\\n# remove outliers\\noutliers_removed = [x for x in df_no_outliers if x >= lower and x <= upper]\\nprint('Non-outlier observations: %d' % len(outliers_removed))\\n\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "df_no_outliers = df.loc[:, ['Age',\t'Height',\t'Weight'\t,'Veg_Consump'\t,'Water_Consump','Meal_Count',\t'H_Cal_Burn',\t'Phys_Act',\t'Time_E_Dev']]\n",
        "df_no_outliers = df_no_outliers.astype(np.float64)\n",
        "df_no_outliers.sc\n",
        "\n",
        "data_mean, data_std = df_no_outliers.mean(), df_no_outliers.std()\n",
        "# identify outliers\n",
        "cut_off = data_std * 3\n",
        "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
        "print(lower)\n",
        "# identify outliers\n",
        "outliers = [x for x in df_no_outliers if x < lower or x > upper]\n",
        "print('Identified outliers: %d' % len(outliers))\n",
        "# remove outliers\n",
        "outliers_removed = [x for x in df_no_outliers if x >= lower and x <= upper]\n",
        "print('Non-outlier observations: %d' % len(outliers_removed))\n",
        "'''"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pYo4JuC91YDU"
      },
      "source": [
        "## Using Decision Tree Classifier, with 'entropy' criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDYdYCcyX1M1",
        "outputId": "17fb903e-21b5-4f02-8b28-f51c4586c4a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using a Decision Tree without scaling is 0.9931561387533195\n",
            "mean scores using a Decision Tree with scaling is 0.9910530005020746\n"
          ]
        }
      ],
      "source": [
        "#numeric_columns = ['Age',\t'Height',\t'Weight'\t,'Veg_Consump'\t,'Water_Consump','Meal_Count',\t'H_Cal_Burn',\t'Phys_Act',\t'Time_E_Dev']\n",
        "\n",
        "trainDF = df.loc[:,df.columns != 'Body_Level']\n",
        "labelDF = df['Body_Level']\n",
        "trainData_not_scaled = trainDF\n",
        "trainData_scaled = StandardScaler().fit_transform(trainDF)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_not_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "DecTree = DecisionTreeClassifier(criterion = 'entropy')\n",
        "scores = cross_val_score(DecTree, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Decision Tree without scaling is {scores.mean()}\")\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "DecTree = DecisionTreeClassifier(criterion = 'entropy')\n",
        "scores = cross_val_score(DecTree, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Decision Tree with scaling is {scores.mean()}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sPchexi21iJU"
      },
      "source": [
        "## same Decision Tree but using Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNYItKAed5QN",
        "outputId": "6169dbcd-51c9-4a70-b0e7-b403b72cf67e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using a Bagged Decision Tree with scaling is 0.9926606480838629\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      1.00      1.00       210\n",
            "           2       0.99      0.99      0.99       217\n",
            "           3       1.00      0.99      0.99       191\n",
            "           4       1.00      1.00      1.00       198\n",
            "\n",
            "    accuracy                           1.00       816\n",
            "   macro avg       1.00      1.00      1.00       816\n",
            "weighted avg       1.00      1.00      1.00       816\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bag = BaggingClassifier(estimator= DecisionTreeClassifier(criterion='entropy'), n_estimators=100, oob_score=True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "scores = cross_val_score(bag, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Bagged Decision Tree with scaling is {scores.mean()}\")\n",
        "bag.fit(X_train, Y_train)\n",
        "print(classification_report(Y_test, bag.predict(X_test)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q4p_9-E61niE"
      },
      "source": [
        "## Using Random forest with scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRnAs9eKj3k-",
        "outputId": "717a095b-595a-41fd-972e-797abb7f3188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using a Random Forest with scaling is 0.9968444590179342\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      1.00      1.00       200\n",
            "           2       1.00      0.99      0.99       196\n",
            "           3       1.00      1.00      1.00       207\n",
            "           4       1.00      1.00      1.00       213\n",
            "\n",
            "    accuracy                           1.00       816\n",
            "   macro avg       1.00      1.00      1.00       816\n",
            "weighted avg       1.00      1.00      1.00       816\n",
            "\n"
          ]
        }
      ],
      "source": [
        "RF = RandomForestClassifier(criterion='entropy')\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "scores = cross_val_score(RF, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Random Forest with scaling is {scores.mean()}\")\n",
        "RF.fit(X_train, Y_train)\n",
        "print(classification_report(Y_test,RF.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "uDY7VEygk9Ot",
        "outputId": "6baaf511-8b8b-451f-9179-a27baeb9e578"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nplaybag = BaggingClassifier(estimator= RandomForestClassifier(criterion=\\'entropy\\'), n_estimators=100, oob_score=True)\\nX_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\\nscores = cross_val_score(playbag, X_train, Y_train, cv=10)\\nprint(f\"mean scores using a Bagged Random Forest with scaling is {scores.mean()}\")\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "playbag = BaggingClassifier(estimator= RandomForestClassifier(criterion='entropy'), n_estimators=100, oob_score=True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "scores = cross_val_score(playbag, X_train, Y_train, cv=10)\n",
        "print(f\"mean scores using a Bagged Random Forest with scaling is {scores.mean()}\")\n",
        "'''"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "74uhzFp110L1"
      },
      "source": [
        "## Same Decision tree and random forest, but using specific Columns, not all of them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS0EfCOLr4JH",
        "outputId": "a41ffc95-cb44-475a-c674-aa0cb49eb710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using a Decision Tree without scaling using SPECIFIC FEATURES is 0.9931715178682825\n",
            "mean scores using a Decision Tree with scaling using SPECIFIC FEATURES is 0.9915814761641739\n"
          ]
        }
      ],
      "source": [
        "#numeric_columns = ['Age',\t'Height',\t'Weight'\t,'Veg_Consump'\t,'Water_Consump','Meal_Count',\t'H_Cal_Burn',\t'Phys_Act',\t'Time_E_Dev']\n",
        "trainDF = df.loc[:,['bmi', 'Fam_Hist', 'Food_Between_Meals', 'Weight', 'Height', 'Age']]\n",
        "labelDF = df['Body_Level']\n",
        "trainData_not_scaled = trainDF\n",
        "trainData_scaled = StandardScaler().fit_transform(trainDF)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_not_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "DecTree = DecisionTreeClassifier(criterion = 'entropy')\n",
        "scores = cross_val_score(DecTree, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Decision Tree without scaling using SPECIFIC FEATURES is {scores.mean()}\")\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "DecTree = DecisionTreeClassifier(criterion = 'entropy')\n",
        "scores = cross_val_score(DecTree, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Decision Tree with scaling using SPECIFIC FEATURES is {scores.mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPmEHqa-tqlW",
        "outputId": "e1de32d3-8fb7-4460-8748-53ecc37c1c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using a Bagged Decision Tree with scaling using SPECIFIC FEATURES is 0.992640350939412\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      0.99      1.00       225\n",
            "           2       0.98      0.99      0.99       198\n",
            "           3       0.98      0.99      0.99       198\n",
            "           4       1.00      0.99      0.99       195\n",
            "\n",
            "    accuracy                           0.99       816\n",
            "   macro avg       0.99      0.99      0.99       816\n",
            "weighted avg       0.99      0.99      0.99       816\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bag = BaggingClassifier(estimator= DecisionTreeClassifier(criterion='entropy'), n_estimators=100, oob_score=True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "scores = cross_val_score(bag, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Bagged Decision Tree with scaling using SPECIFIC FEATURES is {scores.mean()}\")\n",
        "bag.fit(X_train, Y_train)\n",
        "print(classification_report(Y_test,bag.predict(X_test)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwcc7Gn518Nc"
      },
      "source": [
        "## Trying to remove some outliers (top and bottom 10%) from each class, in a trial to remove noise (misclassified data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzcS9prawXkN",
        "outputId": "3b2d2686-584d-42e2-cbda-b012b5daf5e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size before : 680   size after : 510     removed 170 outliers\n",
            "size before : 680   size after : 419     removed 261 outliers\n",
            "size before : 680   size after : 446     removed 234 outliers\n",
            "size before : 680   size after : 463     removed 217 outliers\n"
          ]
        }
      ],
      "source": [
        "df1 = df.loc[df['Body_Level']==1, :]\n",
        "df2 = df.loc[df['Body_Level']==2, :]\n",
        "df3 = df.loc[df['Body_Level']==3, :]\n",
        "df4 = df.loc[df['Body_Level']==4, :]\n",
        "\n",
        "def RemoveTopBottomOutliers (df, columns):\n",
        "  for column in columns:\n",
        "    q_low = df[column].quantile(0.1)\n",
        "    q_hi  = df[column].quantile(0.9)\n",
        "    df_dummy = df.loc[(df[column] > q_hi) & (df[column] < q_low),:]\n",
        "    #print(f\"for Column {column}, found and removed {len(df_dummy)} outliers\")\n",
        "    df_filtered = df.loc[(df[column] < q_hi) & (df[column] > q_low), :]\n",
        "  '''\n",
        "  different_columns = [item for item in list(df.columns) if item not in columns]\n",
        "  for col in different_columns:\n",
        "    df_filtered[col] = df[col]\n",
        "  df_filtered.dropna()\n",
        "  '''\n",
        "  print(f\"size before : {len(df)}   size after : {len(df_filtered)}     removed {len(df) - len(df_filtered)} outliers\")\n",
        "  return df_filtered\n",
        "\n",
        "numeric_columns = ['Age',\t'Height',\t'Weight'\t,'Veg_Consump'\t,'Water_Consump','Meal_Count',\t'H_Cal_Burn',\t'Phys_Act',\t'Time_E_Dev']\n",
        "df1 = RemoveTopBottomOutliers(df1, numeric_columns)\n",
        "df2 = RemoveTopBottomOutliers(df2, numeric_columns)\n",
        "df3 = RemoveTopBottomOutliers(df3, numeric_columns)\n",
        "df4 = RemoveTopBottomOutliers(df4, numeric_columns)\n",
        "\n",
        "DF_no_outliers = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
        "DF_no_outliers = DF_no_outliers.sample(frac=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAjYWh3g0pph",
        "outputId": "9f728f98-7e9b-41fd-e300-c8b870ccf5b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using a Decision Tree without scaling but after removing outliers using SPECIFIC FEATURES is 0.9906134982445172\n",
            "mean scores using a Decision Tree with scaling but after removing outliers using SPECIFIC FEATURES is 0.9929986010201102\n"
          ]
        }
      ],
      "source": [
        "#numeric_columns = ['Age',\t'Height',\t'Weight'\t,'Veg_Consump'\t,'Water_Consump','Meal_Count',\t'H_Cal_Burn',\t'Phys_Act',\t'Time_E_Dev']\n",
        "trainDF = DF_no_outliers.loc[:,['bmi', 'Fam_Hist', 'Food_Between_Meals', 'Weight', 'Height', 'Age']]\n",
        "labelDF = DF_no_outliers['Body_Level']\n",
        "trainData_not_scaled = trainDF\n",
        "trainData_scaled = StandardScaler().fit_transform(trainDF)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_not_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "DecTree = DecisionTreeClassifier(criterion = 'entropy')\n",
        "scores = cross_val_score(DecTree, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Decision Tree without scaling but after removing outliers using SPECIFIC FEATURES is {scores.mean()}\")\n",
        "\n",
        " \n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "DecTree = DecisionTreeClassifier(criterion = 'entropy')\n",
        "scores = cross_val_score(DecTree, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Decision Tree with scaling but after removing outliers using SPECIFIC FEATURES is {scores.mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikN_eYOEmiIH",
        "outputId": "84f2e756-bd1d-4d3e-ba11-dd1bc73defdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using a Bagged Decision Tree with scaling but after removing outliers using SPECIFIC FEATURES is 0.9961302768680731\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      1.00      0.99       144\n",
            "           2       0.98      0.98      0.98       128\n",
            "           3       1.00      0.98      0.99       132\n",
            "           4       1.00      1.00      1.00       148\n",
            "\n",
            "    accuracy                           0.99       552\n",
            "   macro avg       0.99      0.99      0.99       552\n",
            "weighted avg       0.99      0.99      0.99       552\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bag = BaggingClassifier(estimator= DecisionTreeClassifier(criterion='entropy'), n_estimators=100, oob_score=True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "scores = cross_val_score(bag, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Bagged Decision Tree with scaling but after removing outliers using SPECIFIC FEATURES is {scores.mean()}\")\n",
        "bag.fit(X_train, Y_train)\n",
        "print(classification_report(Y_test,bag.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYPeFTdfmoQF",
        "outputId": "3fb76947-b90c-4c03-c98c-25b262522eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using a Random Forest with scaling but after removing outliers using SPECIFIC FEATURES is 0.996122091047237\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      1.00      1.00       156\n",
            "           2       0.99      0.99      0.99       122\n",
            "           3       1.00      0.99      0.99       146\n",
            "           4       0.99      1.00      1.00       128\n",
            "\n",
            "    accuracy                           0.99       552\n",
            "   macro avg       0.99      0.99      0.99       552\n",
            "weighted avg       0.99      0.99      0.99       552\n",
            "\n"
          ]
        }
      ],
      "source": [
        "RF_fin = RandomForestClassifier(criterion='entropy')\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "scores = cross_val_score(RF_fin, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Random Forest with scaling but after removing outliers using SPECIFIC FEATURES is {scores.mean()}\")\n",
        "RF_fin.fit(X_train, Y_train)\n",
        "print(classification_report(Y_test, RF_fin.predict(X_test)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmy65sw42M4T"
      },
      "source": [
        "# Doing PCA to use only the best Features, not all of them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWmHVqrC2MZS",
        "outputId": "e752f159-e6f0-43c6-f880-10e7e4602cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using a Decision Tree with PCA without scaling is 0.966389446828656\n",
            "mean scores using a Decision Tree with PCA with scaling is 0.9726522605414243\n"
          ]
        }
      ],
      "source": [
        "pca = PCA(n_components = 5)\n",
        "\n",
        "trainDF = df.loc[:,df.columns != 'Body_Level']\n",
        "\n",
        "pc = pca.fit_transform(trainDF)\n",
        "\n",
        "\n",
        "labelDF = df['Body_Level']\n",
        "trainData_not_scaled = pc\n",
        "trainData_scaled = StandardScaler().fit_transform(pc)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_not_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "DecTree = DecisionTreeClassifier(criterion = 'entropy')\n",
        "scores = cross_val_score(DecTree, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Decision Tree with PCA without scaling is {scores.mean()}\")\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "DecTree = DecisionTreeClassifier(criterion = 'entropy')\n",
        "scores = cross_val_score(DecTree, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Decision Tree with PCA with scaling is {scores.mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk3wGprw3p_S",
        "outputId": "8692dd51-65a1-46c2-e666-4e5972bc8bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using a Bagged Decision Tree with PCA with scaling is 0.9831533931157385\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.99      0.99       194\n",
            "           2       0.99      0.99      0.99       208\n",
            "           3       0.99      0.99      0.99       207\n",
            "           4       0.99      1.00      0.99       207\n",
            "\n",
            "    accuracy                           0.99       816\n",
            "   macro avg       0.99      0.99      0.99       816\n",
            "weighted avg       0.99      0.99      0.99       816\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bag = BaggingClassifier(estimator= DecisionTreeClassifier(criterion='entropy'), n_estimators=100, oob_score=True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.3, shuffle=True)\n",
        "scores = cross_val_score(bag, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using a Bagged Decision Tree with PCA with scaling is {scores.mean()}\")\n",
        "bag.fit(X_train, Y_train)\n",
        "print(classification_report(Y_test, bag.predict(X_test)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T27gigv_4uXU"
      },
      "source": [
        "# PCA sucks "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nl46bEwC49F7"
      },
      "source": [
        "# Now, let's try AdaBoost"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4bZw1dHm5ZLr"
      },
      "source": [
        "## AdaBoosting using random Forest as base estimator"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nKVfgcDE7vh0"
      },
      "source": [
        "this cell I am using specific Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s98fi915AML",
        "outputId": "61664680-531e-4cc6-bdc8-596d01a7090d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using an AbaBoosted Random Forest without scaling using SPECIFIC FEATURES is 0.994490917147429\n",
            "mean scores using an AbaBoosted Random Forest with scaling using SPECIFIC FEATURES is 0.995103698350573\n"
          ]
        }
      ],
      "source": [
        "#numeric_columns = ['Age',\t'Height',\t'Weight'\t,'Veg_Consump'\t,'Water_Consump','Meal_Count',\t'H_Cal_Burn',\t'Phys_Act',\t'Time_E_Dev']\n",
        "trainDF = df.loc[:,['bmi', 'Fam_Hist', 'Food_Between_Meals', 'Weight', 'Height', 'Age']]\n",
        "labelDF = df['Body_Level']\n",
        "trainData_not_scaled = trainDF\n",
        "trainData_scaled = StandardScaler().fit_transform(trainDF)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_not_scaled , labelDF, test_size = 0.4, shuffle=True)\n",
        "ada = AdaBoostClassifier(estimator = RandomForestClassifier(criterion='entropy'), n_estimators=150)\n",
        "scores = cross_val_score(ada, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using an AbaBoosted Random Forest without scaling using SPECIFIC FEATURES is {scores.mean()}\")\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.4, shuffle=True)\n",
        "ada = AdaBoostClassifier(estimator = RandomForestClassifier(criterion='entropy'), n_estimators=150)\n",
        "scores = cross_val_score(ada, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using an AbaBoosted Random Forest with scaling using SPECIFIC FEATURES is {scores.mean()}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yrqCqBR37yKU"
      },
      "source": [
        "this cell I am using all columns as features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1vNBeca6Bhp",
        "outputId": "87f16fb6-bbf2-4170-f662-884db201ba24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using an AbaBoosted Random Forest without scaling is 0.9950909976258814\n",
            "mean scores using an AbaBoosted Random Forest with scaling is 0.9950994373679414\n"
          ]
        }
      ],
      "source": [
        "trainDF = df.loc[:,df.columns != 'Body_Level']\n",
        "labelDF = df['Body_Level']\n",
        "trainData_not_scaled = trainDF\n",
        "trainData_scaled = StandardScaler().fit_transform(trainDF)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_not_scaled , labelDF, test_size = 0.4, shuffle=True)\n",
        "ada = AdaBoostClassifier(estimator = RandomForestClassifier(criterion='entropy'), n_estimators=150)\n",
        "scores = cross_val_score(ada, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using an AbaBoosted Random Forest without scaling is {scores.mean()}\")\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = 0.4, shuffle=True)\n",
        "ada = AdaBoostClassifier(estimator = RandomForestClassifier(criterion='entropy'), n_estimators=150)\n",
        "scores = cross_val_score(ada, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "print(f\"mean scores using an AbaBoosted Random Forest with scaling is {scores.mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM4SVfYOt1qL",
        "outputId": "81bbc762-0343-4fcd-e195-c01022817fb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      0.99      1.00       303\n",
            "           2       0.99      1.00      1.00       284\n",
            "           3       1.00      1.00      1.00       233\n",
            "           4       1.00      1.00      1.00       268\n",
            "\n",
            "    accuracy                           1.00      1088\n",
            "   macro avg       1.00      1.00      1.00      1088\n",
            "weighted avg       1.00      1.00      1.00      1088\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ada.fit(X_train, Y_train)\n",
        "prediction = ada.predict(X_test)\n",
        "print(classification_report(Y_test , prediction))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R--u6TgP_8bV"
      },
      "source": [
        "# **Wrapping it all together**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6NIhgG3ExiYK"
      },
      "outputs": [],
      "source": [
        "def TrainAdaBoostedRF (df, Train_Cols = None, test_size=0.3, n_estimators=150):\n",
        "  df = AdaBoostPreprocessor(df).copy()\n",
        "  df = AdaBoostSMOTE(df).copy()\n",
        "\n",
        "  if(Train_Cols):\n",
        "    trainDF = df.loc[:, Train_Cols]\n",
        "  else:\n",
        "    trainDF = df.loc[:,df.columns != 'Body_Level']\n",
        "  \n",
        "  labelDF = df['Body_Level']\n",
        "  ssc = StandardScaler().fit(trainDF)\n",
        "  trainData_scaled = ssc.transform(trainDF)\n",
        "\n",
        "  with open('../models/ssc.pkl','wb') as f:\n",
        "    pickle.dump(ssc, f)\n",
        "\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(trainData_scaled , labelDF, test_size = test_size, shuffle=True)\n",
        "  ada = AdaBoostClassifier(estimator = RandomForestClassifier(criterion='entropy'), n_estimators=n_estimators)\n",
        "  scores = cross_val_score(ada, X_train, Y_train, cv=10, scoring='f1_weighted')\n",
        "  print(f\"mean scores using an AbaBoosted Random Forest with scaling is {scores.mean()}\")\n",
        "\n",
        "  ada.fit(X_train, Y_train)\n",
        "  prediction = ada.predict(X_test)\n",
        "  print(f\"################################################################## Classification Report for Test Set ##########################################\\n{classification_report(Y_test , prediction)}\")\n",
        "  \n",
        "\n",
        "  with open('../models/adaBoostRF2.pkl','wb') as f:\n",
        "    pickle.dump(ada, f)\n",
        " \n",
        "  models = [ssc, ada]\n",
        "  with open('../models/model.pkl','wb') as f:\n",
        "    pickle.dump(models, f)\n",
        "\n",
        "\n",
        "def InferAdaBoostedRF(df, path):\n",
        "  df = AdaBoostPreprocessor(df, inference=True).copy()\n",
        "  \n",
        "  with open(path,'rb') as f:\n",
        "    models = pickle.load(f)\n",
        "  ssc = models[0]\n",
        "  ada = models[1]\n",
        "  df = ssc.transform(df).copy()\n",
        "\n",
        "  prediction = ada.predict(df)\n",
        "\n",
        "  ListBodyLevel = ['Body Level 1', 'Body Level 2', 'Body Level 3', 'Body Level 4']\n",
        "  final = []\n",
        "  for i in prediction:\n",
        "    final.append(ListBodyLevel[int(i)-1])\n",
        "  #final = prediction.apply(lambda x: ListBodyLevel[int(x)])\n",
        "  return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMste8b8y4d6",
        "outputId": "605679f3-e2a8-47e6-e580-d7e85ef1b4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean scores using an AbaBoosted Random Forest with scaling is 0.9963198562339016\n",
            "################################################################## Classification Report for Test Set ##########################################\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00       209\n",
            "           2       0.99      1.00      0.99       205\n",
            "           3       0.99      0.98      0.99       200\n",
            "           4       1.00      1.00      1.00       202\n",
            "\n",
            "    accuracy                           0.99       816\n",
            "   macro avg       0.99      0.99      0.99       816\n",
            "weighted avg       0.99      0.99      0.99       816\n",
            "\n"
          ]
        }
      ],
      "source": [
        "finaldf = pd.read_csv('../body_level_classification_train.csv')\n",
        "#features = ['bmi', 'Fam_Hist', 'Food_Between_Meals', 'Weight', 'Height', 'Age']\n",
        "TrainAdaBoostedRF(finaldf, None, 0.3, 150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UHm04z7z0Gt",
        "outputId": "5388fca2-90fa-4ae8-8d21-c72d1125697f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Body Level 4    680\n",
              "Body Level 3    404\n",
              "Body Level 2    203\n",
              "Body Level 1    190\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = pd.read_csv('../body_level_classification_train.csv')\n",
        "#test = test.sample(frac = 0.01)\n",
        "true_labels = test['Body_Level']\n",
        "test = test.drop(columns=['Body_Level'])\n",
        "\n",
        "predicted = pd.DataFrame(InferAdaBoostedRF(test , '../models/model.pkl'))\n",
        "predicted.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5GIzITWBAlx",
        "outputId": "1330bd8b-64df-4087-f40b-2a03f249356f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Body_Level  \n",
            "Body Level 4    680\n",
            "Body Level 3    406\n",
            "Body Level 2    201\n",
            "Body Level 1    190\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#distribution of true values\n",
        "print(pd.DataFrame(true_labels).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa3Y2tTyBJao"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M5HyvA9HWKaJ"
      },
      "source": [
        "# Finishing this NoteBook with some Analysis\n",
        "\n",
        "1.   Bias-Variance Trade Off\n",
        "2.   Learning Curve of my used models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "c0IRyZmofOz6",
        "outputId": "83dcc7b1-d540-4f0f-e8d6-1c9a76b6eea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index 1534 is out of bounds for axis 0 with size 816",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m X_test \u001b[39m=\u001b[39m X_test\u001b[39m.\u001b[39mvalues\n\u001b[0;32m     24\u001b[0m y_test \u001b[39m=\u001b[39m y_test\u001b[39m.\u001b[39mvalues\n\u001b[1;32m---> 25\u001b[0m avg_expected_loss, avg_bias, avg_var \u001b[39m=\u001b[39m bias_variance_decomp(estimator\u001b[39m=\u001b[39;49mada, X_train\u001b[39m=\u001b[39;49mX_train,y_train\u001b[39m=\u001b[39;49my_train, X_test\u001b[39m=\u001b[39;49mX_test,  y_test\u001b[39m=\u001b[39;49my_test, loss\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m0-1_loss\u001b[39;49m\u001b[39m'\u001b[39;49m, random_seed\u001b[39m=\u001b[39;49m\u001b[39m123\u001b[39;49m)\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAverage expected loss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m avg_expected_loss)\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAverage bias: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m avg_bias)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlxtend\\evaluate\\bias_variance_decomp.py:106\u001b[0m, in \u001b[0;36mbias_variance_decomp\u001b[1;34m(estimator, X_train, y_train, X_test, y_test, loss, num_rounds, random_seed, **fit_params)\u001b[0m\n\u001b[0;32m    103\u001b[0m all_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((num_rounds, y_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_rounds):\n\u001b[1;32m--> 106\u001b[0m     X_boot, y_boot \u001b[39m=\u001b[39m _draw_bootstrap_sample(rng, X_train, y_train)\n\u001b[0;32m    108\u001b[0m     \u001b[39m# Keras support\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m estimator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mSequential\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFunctional\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    110\u001b[0m         \u001b[39m# reset model\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlxtend\\evaluate\\bias_variance_decomp.py:16\u001b[0m, in \u001b[0;36m_draw_bootstrap_sample\u001b[1;34m(rng, X, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m sample_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m     13\u001b[0m bootstrap_indices \u001b[39m=\u001b[39m rng\u001b[39m.\u001b[39mchoice(\n\u001b[0;32m     14\u001b[0m     sample_indices, size\u001b[39m=\u001b[39msample_indices\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], replace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m \u001b[39mreturn\u001b[39;00m X[bootstrap_indices], y[bootstrap_indices]\n",
            "\u001b[1;31mIndexError\u001b[0m: index 1534 is out of bounds for axis 0 with size 816"
          ]
        }
      ],
      "source": [
        "#Bias Variance Decomp\n",
        "\n",
        "\n",
        "X = pd.read_csv('../body_level_classification_train.csv')\n",
        "X = AdaBoostPreprocessor(X).copy()\n",
        "X = AdaBoostSMOTE(X).copy()\n",
        "\n",
        "y=X['Body_Level'].copy()\n",
        "X.drop(columns=['Body_Level'], inplace=True)\n",
        "with open('../models/model.pkl', 'rb') as f:\n",
        "  models= pickle.load(f)\n",
        "  ssc = models[0]\n",
        "  ada = models[1]\n",
        "\n",
        "X = ssc.transform(X).copy()\n",
        "\n",
        "\n",
        "X_train,y_train,X_test, y_test = train_test_split(X,y, test_size = 0.3)\n",
        "print(type(X_train))\n",
        "print(type(y_train))\n",
        "print(type(X_test))\n",
        "print(type(y_test))\n",
        "X_test = X_test.values\n",
        "y_test = y_test.values\n",
        "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(estimator=ada, X_train=X_train,y_train=y_train, X_test=X_test,  y_test=y_test, loss='0-1_loss', random_seed=123)\n",
        "\n",
        "print('Average expected loss: %.3f' % avg_expected_loss)\n",
        "print('Average bias: %.3f' % avg_bias)\n",
        "print('Average variance: %.3f' % avg_var)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82itSedfWWmg"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv('../body_level_classification_train.csv')\n",
        "X = AdaBoostPreprocessor(X).copy()\n",
        "X = AdaBoostSMOTE(X).copy()\n",
        "\n",
        "y=X['Body_Level'].copy()\n",
        "X.drop(columns=['Body_Level'], inplace=True)\n",
        "with open('../models/model.pkl', 'rb') as f:\n",
        "  models= pickle.load(f)\n",
        "  ssc = models[0]\n",
        "  ada = models[1]\n",
        "\n",
        "\n",
        "common_params = {\n",
        "    \"X\": X,\n",
        "    \"y\": y,\n",
        "    \"train_sizes\": np.linspace(0.1, 1.0, 5),\n",
        "    \"cv\": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),\n",
        "    \"n_jobs\": 4,\n",
        "    \"return_times\": True,\n",
        "}\n",
        "\n",
        "train_sizes, _, test_scores_ada, fit_times_ada, score_times_ada = learning_curve(\n",
        "    ada, **common_params\n",
        ")\n",
        "train_sizes, _, test_scores_RF, fit_times_RF, score_times_RF = learning_curve(\n",
        "    RF_fin, **common_params\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
        "\n",
        "for ax_idx, (fit_times, test_scores, estimator) in enumerate(\n",
        "    zip(\n",
        "        [fit_times_ada, fit_times_RF],\n",
        "        [test_scores_ada, test_scores_RF],\n",
        "        [ada, RF_fin],\n",
        "    )\n",
        "):\n",
        "    ax[ax_idx].plot(fit_times.mean(axis=1), test_scores.mean(axis=1), \"o-\")\n",
        "    ax[ax_idx].fill_between(\n",
        "        fit_times.mean(axis=1),\n",
        "        test_scores.mean(axis=1) - test_scores.std(axis=1),\n",
        "        test_scores.mean(axis=1) + test_scores.std(axis=1),\n",
        "        alpha=0.3,\n",
        "    )\n",
        "    ax[ax_idx].set_ylabel(\"Accuracy\")\n",
        "    ax[ax_idx].set_xlabel(\"Fit time (s)\")\n",
        "    ax[ax_idx].set_title(\n",
        "        f\"Performance of the {estimator.__class__.__name__} classifier\"\n",
        "    )\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxNCl6cMZS-W"
      },
      "outputs": [],
      "source": [
        "\n",
        "common_params = {\n",
        "    \"X\": X,\n",
        "    \"y\": y,\n",
        "    \"train_sizes\": np.linspace(0.1, 1.0, 5),\n",
        "    \"cv\": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),\n",
        "    \"n_jobs\": 4,\n",
        "    \"return_times\": True,\n",
        "}\n",
        "\n",
        "train_sizes, _, test_scores_ada, fit_times_ada, score_times_ada = learning_curve(\n",
        "    ada, **common_params\n",
        ")\n",
        "train_sizes, _, test_scores_RF, fit_times_RF, score_times_RF = learning_curve(\n",
        "    RF_fin, **common_params\n",
        ")\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 12), sharex=True)\n",
        "\n",
        "for ax_idx, (fit_times, score_times, estimator) in enumerate(\n",
        "    zip(\n",
        "        [fit_times_ada, fit_times_RF],\n",
        "        [score_times_ada, score_times_RF],\n",
        "        [ada, RF_fin],\n",
        "    )\n",
        "):\n",
        "    # scalability regarding the fit time\n",
        "    ax[0, ax_idx].plot(train_sizes, fit_times.mean(axis=1), \"o-\")\n",
        "    ax[0, ax_idx].fill_between(\n",
        "        train_sizes,\n",
        "        fit_times.mean(axis=1) - fit_times.std(axis=1),\n",
        "        fit_times.mean(axis=1) + fit_times.std(axis=1),\n",
        "        alpha=0.3,\n",
        "    )\n",
        "    ax[0, ax_idx].set_ylabel(\"Fit time (s)\")\n",
        "    ax[0, ax_idx].set_title(\n",
        "        f\"Scalability of the {estimator.__class__.__name__} classifier\"\n",
        "    )\n",
        "\n",
        "    # scalability regarding the score time\n",
        "    ax[1, ax_idx].plot(train_sizes, score_times.mean(axis=1), \"o-\")\n",
        "    ax[1, ax_idx].fill_between(\n",
        "        train_sizes,\n",
        "        score_times.mean(axis=1) - score_times.std(axis=1),\n",
        "        score_times.mean(axis=1) + score_times.std(axis=1),\n",
        "        alpha=0.3,\n",
        "    )\n",
        "    ax[1, ax_idx].set_ylabel(\"Score time (s)\")\n",
        "    ax[1, ax_idx].set_xlabel(\"Number of training samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr2DnUlfbCHD"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 6), sharey=True)\n",
        "\n",
        "common_params = {\n",
        "    \"X\": X,\n",
        "    \"y\": y,\n",
        "    \"train_sizes\": np.linspace(0.1, 1.0, 5),\n",
        "    \"cv\": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),\n",
        "    \"score_type\": \"both\",\n",
        "    \"n_jobs\": 4,\n",
        "    \"line_kw\": {\"marker\": \"o\"},\n",
        "    \"std_display_style\": \"fill_between\",\n",
        "    \"score_name\": \"Accuracy\",\n",
        "}\n",
        "\n",
        "for ax_idx, estimator in enumerate([ada, RF_fin]):\n",
        "    LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[ax_idx])\n",
        "    handles, label = ax[ax_idx].get_legend_handles_labels()\n",
        "    ax[ax_idx].legend(handles[:2], [\"Training Score\", \"Test Score\"])\n",
        "    ax[ax_idx].set_title(f\"Learning Curve for {estimator.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G67i5W8gV2n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
